{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wolof NLP - Evaluation\n",
    "\n",
    "This notebook evaluates the tokenizer on a gold standard corpus of 542 annotated Wolof sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from wolof_nlp import WolofTokenizer\n",
    "\n",
    "print(\"Loading gold standard...\")\n",
    "with open('../data/gold_standard.json') as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "# Clean gold standard (remove empty tokens)\n",
    "for sent in gold:\n",
    "    sent['tokens'] = [t.lower() for t in sent['tokens'] if t.strip()]\n",
    "\n",
    "print(f\"Loaded {len(gold)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gold Standard Overview\n",
    "\n",
    "The gold standard contains sentences from Senegalese YouTube comments, categorized by difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "categories = {}\n",
    "for sent in gold:\n",
    "    cat = sent.get('category', 'unknown')\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"Category distribution:\")\n",
    "for cat, count in sorted(categories.items()):\n",
    "    print(f\"  {cat:<15} {count:>4} sentences ({count/len(gold)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nSample sentences:\")\n",
    "for sent in gold[:3]:\n",
    "    print(f\"  {sent['text']:<30} -> {sent['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tokenizer(tokenize_func, name):\n",
    "    \"\"\"Evaluate a tokenizer against gold standard.\"\"\"\n",
    "    exact_match = 0\n",
    "    total_correct = 0\n",
    "    total_pred = 0\n",
    "    total_gold = 0\n",
    "    errors = []\n",
    "    \n",
    "    for sent in gold:\n",
    "        expected = sent['tokens']\n",
    "        try:\n",
    "            predicted = tokenize_func(sent['text'])\n",
    "        except Exception as e:\n",
    "            predicted = []\n",
    "        \n",
    "        total_gold += len(expected)\n",
    "        total_pred += len(predicted)\n",
    "        \n",
    "        # Token-level matching\n",
    "        for tok in predicted:\n",
    "            if tok in expected:\n",
    "                total_correct += 1\n",
    "        \n",
    "        # Exact match\n",
    "        if predicted == expected:\n",
    "            exact_match += 1\n",
    "        elif len(errors) < 20:\n",
    "            errors.append({\n",
    "                'text': sent['text'],\n",
    "                'expected': expected,\n",
    "                'predicted': predicted,\n",
    "                'category': sent.get('category', 'unknown')\n",
    "            })\n",
    "    \n",
    "    precision = total_correct / total_pred if total_pred else 0\n",
    "    recall = total_correct / total_gold if total_gold else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'exact_match': exact_match,\n",
    "        'total': len(gold),\n",
    "        'errors': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Comparison\n",
    "\n",
    "Compare Wolof NLP tokenizer against standard baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wolof NLP tokenizer\n",
    "tokenizer = WolofTokenizer(normalize=True, detect_language=True, segment_attached=True)\n",
    "def wolof_tokenize(text):\n",
    "    return [t.text.lower() for t in tokenizer.tokenize(text) \n",
    "            if t.type.name in ('WORD', 'PUNCTUATION')]\n",
    "\n",
    "# Regex tokenizer (similar to NLTK RegexpTokenizer)\n",
    "def regex_tokenize(text):\n",
    "    return [t.lower() for t in re.findall(r'\\w+|[^\\w\\s]', text)]\n",
    "\n",
    "# Whitespace tokenizer\n",
    "def whitespace_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Run evaluation\n",
    "results = [\n",
    "    evaluate_tokenizer(wolof_tokenize, \"Wolof NLP\"),\n",
    "    evaluate_tokenizer(regex_tokenize, \"Regex (\\\\w+)\"),\n",
    "    evaluate_tokenizer(whitespace_tokenize, \"Whitespace\"),\n",
    "]\n",
    "\n",
    "# Print results\n",
    "print(f\"{'System':<20} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Exact Match':>15}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    em_str = f\"{r['exact_match']}/{r['total']} ({r['exact_match']/r['total']*100:.1f}%)\"\n",
    "    print(f\"{r['name']:<20} {r['precision']*100:>9.1f}% {r['recall']*100:>9.1f}% {r['f1']*100:>9.1f}% {em_str:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate by category\n",
    "cat_results = {}\n",
    "for sent in gold:\n",
    "    cat = sent.get('category', 'unknown')\n",
    "    if cat not in cat_results:\n",
    "        cat_results[cat] = {'correct': 0, 'total': 0}\n",
    "    \n",
    "    cat_results[cat]['total'] += 1\n",
    "    expected = sent['tokens']\n",
    "    predicted = wolof_tokenize(sent['text'])\n",
    "    if predicted == expected:\n",
    "        cat_results[cat]['correct'] += 1\n",
    "\n",
    "print(f\"{'Category':<15} {'Correct':>10} {'Total':>10} {'Accuracy':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for cat, data in sorted(cat_results.items()):\n",
    "    acc = data['correct'] / data['total'] * 100\n",
    "    print(f\"{cat:<15} {data['correct']:>10} {data['total']:>10} {acc:>11.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get errors from Wolof NLP\n",
    "wolof_result = results[0]\n",
    "errors = wolof_result['errors']\n",
    "\n",
    "print(f\"Sample errors ({len(errors)} shown):\\n\")\n",
    "for err in errors[:10]:\n",
    "    print(f\"Text:     {err['text']}\")\n",
    "    print(f\"Expected: {err['expected']}\")\n",
    "    print(f\"Got:      {err['predicted']}\")\n",
    "    print(f\"Category: {err['category']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize errors\n",
    "error_types = {\n",
    "    'code_switch': 0,\n",
    "    'punctuation': 0,\n",
    "    'verbal_suffix': 0,\n",
    "    'other': 0\n",
    "}\n",
    "\n",
    "for err in errors:\n",
    "    text = err['text'].lower()\n",
    "    if err['category'] == 'code_switch':\n",
    "        error_types['code_switch'] += 1\n",
    "    elif err['category'] == 'punctuation':\n",
    "        error_types['punctuation'] += 1\n",
    "    elif any(s in text for s in ['oo', 'loo', 'uloo']):\n",
    "        error_types['verbal_suffix'] += 1\n",
    "    else:\n",
    "        error_types['other'] += 1\n",
    "\n",
    "total_errors = sum(error_types.values())\n",
    "print(\"Error type distribution:\")\n",
    "for etype, count in sorted(error_types.items(), key=lambda x: -x[1]):\n",
    "    pct = count / total_errors * 100 if total_errors else 0\n",
    "    print(f\"  {etype:<20} {count:>4} ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "The Wolof NLP tokenizer significantly outperforms generic baselines because it handles:\n",
    "\n",
    "1. **Morpheme segmentation**: Splits TAM markers (e.g., `damay` â†’ `da ma y`)\n",
    "2. **Orthography normalization**: Handles spelling variations\n",
    "3. **Language-aware processing**: Doesn't incorrectly segment French words\n",
    "\n",
    "Main error sources:\n",
    "- Code-switching boundaries\n",
    "- Complex verbal suffixes (-oo, -loo)\n",
    "- Punctuation handling edge cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
